{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "import MySQLdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = MySQLdb.connect(host=\"127.0.0.1\", port=3309, user=\"root\", passwd=\"root\", db=\"tweets\")\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_tweets_text(user_id, data_type, num=200):\n",
    "    \"\"\"returns list of tweets as dicts\"\"\"\n",
    "    cmd = \"\"\"\n",
    "    SELECT timeline\n",
    "    FROM {}\n",
    "    WHERE user_id = %s;\n",
    "    \"\"\".format('tweets_%s' % (data_type))\n",
    "    cursor.execute(cmd, (user_id,))\n",
    "    timeline = json.loads(cursor.fetchone()[0])\n",
    "    return [msg['text'] for msg in timeline if not 'RT' in msg['text']][:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    \"\"\"returns list of words\"\"\"\n",
    "    text_nolinks = re.sub(r'http[^ ]+', '', text)\n",
    "    words = re.split(r'[^\\w#]', text_nolinks)\n",
    "#     words = re.split(r'[\\W]', text_nolinks)\n",
    "    return [word for word in words if len(word) > 0 and not word.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(words):\n",
    "    \"\"\"returns list of tokens\"\"\"\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        tmp = stemmer.stem(word.lower())\n",
    "        tmp = wnl.lemmatize(tmp)\n",
    "        if tmp not in stopwords:\n",
    "            tokens.append(tmp)\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweet_tokens(tweet):\n",
    "    return get_tokens(get_words(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_users_tokens(df_users, data_type, tweets_num=200):\n",
    "    \"\"\"returns users list and list of user dicts. Each dict contains frequence of user tokens\"\"\"\n",
    "    users = df_users['twitter_id'].values\n",
    "    user_dicts = []\n",
    "    leng = users.size\n",
    "    for i, user in enumerate(users):\n",
    "        tokens = []\n",
    "        \n",
    "        sys.stdout.write(\"\\rProgress: %.2f%% %d          \" % (100.0 * (i + 1) / leng, user))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tweets = get_user_tweets_text(user, data_type, num=tweets_num)\n",
    "\n",
    "        for tweet in tweets:\n",
    "            tokens += get_tweet_tokens(tweet)\n",
    "\n",
    "        uniq_tokens = np.unique(tokens, return_counts=True)\n",
    "        user_dicts.append(dict(zip(uniq_tokens[0], uniq_tokens[1])))\n",
    "        \n",
    "        del(tokens)\n",
    "    return users, user_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.83% 357859404          "
     ]
    }
   ],
   "source": [
    "TRAINING_SET_URL = \"../twitter_train.csv\"\n",
    "TESTING_SET_URL = \"../twitter_test.csv\"\n",
    "df_train = pd.read_csv(TRAINING_SET_URL)\n",
    "df_test = pd.read_csv(TESTING_SET_URL)\n",
    "\n",
    "_time = time.time()\n",
    "users_tr, users_tr_tokens = collect_users_tokens(df_train, 'train', tweets_num=300)\n",
    "users_te, users_te_tokens = collect_users_tokens(df_test, 'test', tweets_num=300)\n",
    "print(time.time() - _time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.close()\n",
    "np.savez(\"train.dat\", users=users_tr, users_tokens=users_tr_tokens)\n",
    "np.savez(\"test.dat\", users=users_te, users_tokens=users_te_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# v = DictVectorizer()\n",
    "# vs = v.fit_transform(users_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
